{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF QA  RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from operator import itemgetter\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_file(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "\n",
    "    return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, PINECONE_API_KEY):\n",
    "      \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        pc.delete_index(index_name) # To avoid any conflicts in retrieval\n",
    "    pc.create_index(\n",
    "                name=index_name, \n",
    "                dimension=384, \n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return index_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_response(index, question, model):\n",
    "    retriever = index.as_retriever()\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    chain = model | parser \n",
    "\n",
    "    template = \"\"\"\n",
    "    You must provide an answer based strictly on the context below. The answer is highly likely to be found within the given context, so analyze it thoroughly before responding. Only if there is absolutely no relevant information, respond with \"I don't know\".\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    prompt.format(context=\"Here is some context\", question=\"Here is a question\")\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | parser\n",
    "    )\n",
    "    matching_results=index.similarity_search(question,k=2)\n",
    "\n",
    "    return f\"Answer: {chain.invoke({'question': question})}\", matching_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "from utilis import load_split_file, create_index, final_response\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "SAVE_DIR = \"/RAG-APP/data.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatMistralAI(mistral_api_key=MISTRAL_API_KEY)\n",
    "pinecone_index = \"index\"\n",
    "index_name = create_index(pinecone_index, PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/last lesson.pdf\"\n",
    "docs = load_split_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = LangchainPinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "question = \"What data does google collects?\"\n",
    "matching_results=index.similarity_search(question,k=2)\n",
    "\n",
    "answer = final_response(index, question, model)\n",
    "\n",
    "print(f\"{answer}\\n\\n{matching_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
