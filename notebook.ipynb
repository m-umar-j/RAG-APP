{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF QA  RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from operator import itemgetter\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_file(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "\n",
    "    return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, PINECONE_API_KEY):\n",
    "      \n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        pc.delete_index(index_name) # To avoid any conflicts in retrieval\n",
    "    pc.create_index(\n",
    "                name=index_name, \n",
    "                dimension=384, \n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return index_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_response(index, question, model):\n",
    "    retriever = index.as_retriever()\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    chain = model | parser \n",
    "\n",
    "    template = \"\"\"\n",
    "    You must provide an answer based strictly on the context below. The answer is highly likely to be found within the given context, so analyze it thoroughly before responding. Only if there is absolutely no relevant information, respond with \"I don't know\".\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    prompt.format(context=\"Here is some context\", question=\"Here is a question\")\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | parser\n",
    "    )\n",
    "    matching_results=index.similarity_search(question,k=2)\n",
    "\n",
    "    return f\"Answer: {chain.invoke({'question': question})}\", matching_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/rag-app/RAG-APP/myvenv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "from utilis import load_split_file, create_index, final_response\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "SAVE_DIR = \"/RAG-APP/data.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_559475/666187351.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/home/azureuser/rag-app/RAG-APP/myvenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatMistralAI(mistral_api_key=MISTRAL_API_KEY)\n",
    "pinecone_index = \"index\"\n",
    "index_name = create_index(pinecone_index, PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/last lesson.pdf\"\n",
    "docs = load_split_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = LangchainPinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "question = \"What data does google collects?\"\n",
    "matching_results=index.similarity_search(question,k=2)\n",
    "\n",
    "answer = final_response(index, question, model)\n",
    "\n",
    "print(f\"{answer}\\n\\n{matching_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
